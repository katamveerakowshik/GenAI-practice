{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c775f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a calculator and a knowledge base \n",
    "\n",
    "def calculator(expression):\n",
    "    try:\n",
    "        return str(eval(expression))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "knowledge_base = {\n",
    "    \"LangChain\": \"A framework for building LLM-powered agents.\",\n",
    "    \"RAG\": \"Retrieval-Augmented Generation, which uses external knowledge to improve answers.\"\n",
    "}\n",
    "\n",
    "def lookup(term):\n",
    "    return knowledge_base.get(term, \"Term not found in knowledge base.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57f8fa72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf491f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReAct Framework Implementation: It stands for reasoning and acting.\n",
    "\n",
    "def react_agent(query, max_turns = 5):\n",
    "    # Initialize conversation with system prompt\n",
    "    conversation = [\n",
    "        {\"role\":\"system\",\"content\":\"You are a ReAct agent. You must think step by step and take actions when needed. Format:\\nThought: ...\\nAction: tool[input]\\nObservation: ...\\nFinal Answer: ...\"}\n",
    "    ]\n",
    "    conversation.append({\"role\":\"user\",\"content\":query})\n",
    "\n",
    "    for i in range(max_turns):\n",
    "        llm = HuggingFaceEndpoint(\n",
    "            repo_id=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "            task=\"text-generation\",\n",
    "            temperature=0)\n",
    "        chat = ChatHuggingFace(llm=llm)\n",
    "        resp = chat.invoke(conversation)\n",
    "        print(resp.content)\n",
    "\n",
    "        if \"Action\" in resp.content:\n",
    "            if \"calculator[\" in resp.content:\n",
    "                expr = resp.content.split(\"calculator[\")[1].split(\"]\")[0]\n",
    "                obs = calculator(expr)\n",
    "            \n",
    "            elif \"lookup[\" in resp.content:\n",
    "                term = resp.content.split(\"lookup[\")[1].split(\"]\")[0]\n",
    "                obs = lookup(term)\n",
    "\n",
    "            else:\n",
    "                obs = \"Unknown action.\"\n",
    "\n",
    "            conversation.append({\"role\":\"assistant\",\"content\":resp.content})\n",
    "            conversation.append({\"role\":\"user\",\"content\":f\"Observation: {obs}\"})\n",
    "\n",
    "        else:  \n",
    "            break\n",
    "    print(conversation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12f9c071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: First, I need to follow the order of operations (PEMDAS), which means parentheses first, then multiplication and division from left to right, and finally addition and subtraction from left to right.\n",
      "\n",
      "Observation: The expression inside the parentheses is (15*3), which equals 45. The expression inside the second parentheses is (120/6), which equals 20.\n",
      "\n",
      "Action: Now, I will add the results of the two expressions together: 45 + 20 = 65.\n",
      "\n",
      "Final Answer: 65\n",
      "Thought: Since we know the correct order of operations and the calculations, we can redo the calculations without the unknown action in this case.\n",
      "\n",
      "Action: Now, I will add the results of the two expressions together: 15*3 + 120/6 = 45 + 20 = 65.\n",
      "\n",
      "Final Answer: 65\n",
      "Thought: Since the unknown action is still present, I will assume it's asking me to simplify the fraction (120/6) first.\n",
      "\n",
      "Action: To simplify the fraction (120/6), I will divide both the numerator and the denominator by their greatest common divisor, which is 6. This results in (20/1) or simply 20.\n",
      "\n",
      "Now, I will add the result of multiplying 15 by 3 to the simplified fraction: 15*3 + 20 = 45 + 20 = 65.\n",
      "\n",
      "Final Answer: 65\n",
      "Thought: Since we still don't have the unknown action, I will assume it's asking me to calculate the multiplication of 15 and 3 first.\n",
      "\n",
      "Action: Multiply 15 and 3: 15*3 = 45.\n",
      "\n",
      "Now, I will add the result to the simplified fraction: 45 + 20 = 65.\n",
      "\n",
      "Final Answer: 65\n",
      "Thought: Since the unknown action is still present, I will assume it's asking me to follow the order of operations (PEMDAS) again.\n",
      "\n",
      "Action: First, I will divide 120 by 6: 120/6 = 20.\n",
      "\n",
      "Now, I will multiply 15 by the result: 15*20 = 300.\n",
      "\n",
      "Finally, I will add the results together: 300 + 20 = 320.\n",
      "\n",
      "Final Answer: 320\n",
      "[{'role': 'system', 'content': 'You are a ReAct agent. You must think step by step and take actions when needed. Format:\\nThought: ...\\nAction: tool[input]\\nObservation: ...\\nFinal Answer: ...'}, {'role': 'user', 'content': 'What is (15*3) + (120/6)?'}, {'role': 'assistant', 'content': 'Thought: First, I need to follow the order of operations (PEMDAS), which means parentheses first, then multiplication and division from left to right, and finally addition and subtraction from left to right.\\n\\nObservation: The expression inside the parentheses is (15*3), which equals 45. The expression inside the second parentheses is (120/6), which equals 20.\\n\\nAction: Now, I will add the results of the two expressions together: 45 + 20 = 65.\\n\\nFinal Answer: 65'}, {'role': 'user', 'content': 'Observation: Unknown action.'}, {'role': 'assistant', 'content': 'Thought: Since we know the correct order of operations and the calculations, we can redo the calculations without the unknown action in this case.\\n\\nAction: Now, I will add the results of the two expressions together: 15*3 + 120/6 = 45 + 20 = 65.\\n\\nFinal Answer: 65'}, {'role': 'user', 'content': 'Observation: Unknown action.'}, {'role': 'assistant', 'content': \"Thought: Since the unknown action is still present, I will assume it's asking me to simplify the fraction (120/6) first.\\n\\nAction: To simplify the fraction (120/6), I will divide both the numerator and the denominator by their greatest common divisor, which is 6. This results in (20/1) or simply 20.\\n\\nNow, I will add the result of multiplying 15 by 3 to the simplified fraction: 15*3 + 20 = 45 + 20 = 65.\\n\\nFinal Answer: 65\"}, {'role': 'user', 'content': 'Observation: Unknown action.'}, {'role': 'assistant', 'content': \"Thought: Since we still don't have the unknown action, I will assume it's asking me to calculate the multiplication of 15 and 3 first.\\n\\nAction: Multiply 15 and 3: 15*3 = 45.\\n\\nNow, I will add the result to the simplified fraction: 45 + 20 = 65.\\n\\nFinal Answer: 65\"}, {'role': 'user', 'content': 'Observation: Unknown action.'}, {'role': 'assistant', 'content': \"Thought: Since the unknown action is still present, I will assume it's asking me to follow the order of operations (PEMDAS) again.\\n\\nAction: First, I will divide 120 by 6: 120/6 = 20.\\n\\nNow, I will multiply 15 by the result: 15*20 = 300.\\n\\nFinally, I will add the results together: 300 + 20 = 320.\\n\\nFinal Answer: 320\"}, {'role': 'user', 'content': 'Observation: Unknown action.'}]\n"
     ]
    }
   ],
   "source": [
    "react_agent(\"What is (15*3) + (120/6)?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86122d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c37cebcf",
   "metadata": {},
   "source": [
    "There is a small issue here, the model is not even making the tool calls and for each of them it is showing observation is unkown action. And the model is not stopping if it has already has the answer. It's maximum turn are 5, but that does not mean it should use 5 turns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa73665e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m      3\u001b[39m hf = HuggingFacePipeline.from_model_id(\n\u001b[32m      4\u001b[39m     model_id=\u001b[33m\"\u001b[39m\u001b[33mTinyLlama/TinyLlama-1.1B-Chat-v1.0\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     task=\u001b[33m\"\u001b[39m\u001b[33mtext-generation\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     }\n\u001b[32m     11\u001b[39m )\n\u001b[32m     13\u001b[39m prompt = \u001b[33m\"\"\"\u001b[39m\u001b[33m<|system|>\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[33mYou are a helpful assistant.\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33m<|user|>\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[33mWhat is the capital of France?\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[33m<|assistant|>\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mhf\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Katam Veera Kowshik\\OneDrive\\Desktop\\Projects\\udemy\\venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:378\u001b[39m, in \u001b[36mBaseLLM.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    367\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    369\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    374\u001b[39m     **kwargs: Any,\n\u001b[32m    375\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    376\u001b[39m     config = ensure_config(config)\n\u001b[32m    377\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    388\u001b[39m         .generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m    389\u001b[39m         .text\n\u001b[32m    390\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Katam Veera Kowshik\\OneDrive\\Desktop\\Projects\\udemy\\venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:789\u001b[39m, in \u001b[36mBaseLLM.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    780\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    781\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    782\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    786\u001b[39m     **kwargs: Any,\n\u001b[32m    787\u001b[39m ) -> LLMResult:\n\u001b[32m    788\u001b[39m     prompt_strings = [p.to_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Katam Veera Kowshik\\OneDrive\\Desktop\\Projects\\udemy\\venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:1011\u001b[39m, in \u001b[36mBaseLLM.generate\u001b[39m\u001b[34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    992\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    993\u001b[39m     run_managers = [\n\u001b[32m    994\u001b[39m         callback_manager.on_llm_start(\n\u001b[32m    995\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1009\u001b[39m         )\n\u001b[32m   1010\u001b[39m     ]\n\u001b[32m-> \u001b[39m\u001b[32m1011\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) > \u001b[32m0\u001b[39m:\n\u001b[32m   1019\u001b[39m     run_managers = [\n\u001b[32m   1020\u001b[39m         callback_managers[idx].on_llm_start(\n\u001b[32m   1021\u001b[39m             \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1028\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[32m   1029\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Katam Veera Kowshik\\OneDrive\\Desktop\\Projects\\udemy\\venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:815\u001b[39m, in \u001b[36mBaseLLM._generate_helper\u001b[39m\u001b[34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[39m\n\u001b[32m    804\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate_helper\u001b[39m(\n\u001b[32m    805\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    806\u001b[39m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    811\u001b[39m     **kwargs: Any,\n\u001b[32m    812\u001b[39m ) -> LLMResult:\n\u001b[32m    813\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    814\u001b[39m         output = (\n\u001b[32m--> \u001b[39m\u001b[32m815\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    816\u001b[39m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    817\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    818\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[32m    819\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    821\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    822\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[32m    823\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._generate(prompts, stop=stop)\n\u001b[32m    824\u001b[39m         )\n\u001b[32m    825\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    826\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Katam Veera Kowshik\\OneDrive\\Desktop\\Projects\\udemy\\venv\\Lib\\site-packages\\langchain_huggingface\\llms\\huggingface_pipeline.py:332\u001b[39m, in \u001b[36mHuggingFacePipeline._generate\u001b[39m\u001b[34m(self, prompts, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    329\u001b[39m batch_prompts = prompts[i : i + \u001b[38;5;28mself\u001b[39m.batch_size]\n\u001b[32m    331\u001b[39m \u001b[38;5;66;03m# Process batch of prompts\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m responses = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_prompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpipeline_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[38;5;66;03m# Process each response in the batch\u001b[39;00m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m j, response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(responses):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Katam Veera Kowshik\\OneDrive\\Desktop\\Projects\\udemy\\venv\\Lib\\site-packages\\transformers\\pipelines\\text_generation.py:332\u001b[39m, in \u001b[36mTextGenerationPipeline.__call__\u001b[39m\u001b[34m(self, text_inputs, **kwargs)\u001b[39m\n\u001b[32m    330\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(\u001b[38;5;28mlist\u001b[39m(chats), **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Katam Veera Kowshik\\OneDrive\\Desktop\\Projects\\udemy\\venv\\Lib\\site-packages\\transformers\\pipelines\\base.py:1448\u001b[39m, in \u001b[36mPipeline.__call__\u001b[39m\u001b[34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[39m\n\u001b[32m   1444\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[32m   1445\u001b[39m     final_iterator = \u001b[38;5;28mself\u001b[39m.get_iterator(\n\u001b[32m   1446\u001b[39m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[32m   1447\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1448\u001b[39m     outputs = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[32m   1450\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Katam Veera Kowshik\\OneDrive\\Desktop\\Projects\\udemy\\venv\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:126\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_item()\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m item = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m processed = \u001b[38;5;28mself\u001b[39m.infer(item, **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Katam Veera Kowshik\\OneDrive\\Desktop\\Projects\\udemy\\venv\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:126\u001b[39m, in \u001b[36mPipelineIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.loader_batch_item()\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m item = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m processed = \u001b[38;5;28mself\u001b[39m.infer(item, **\u001b[38;5;28mself\u001b[39m.params)\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Katam Veera Kowshik\\OneDrive\\Desktop\\Projects\\udemy\\venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:737\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m737\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprofiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecord_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_profile_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sampler_iter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;49;00m\n\u001b[32m    740\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-arg]\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Katam Veera Kowshik\\OneDrive\\Desktop\\Projects\\udemy\\venv\\Lib\\site-packages\\torch\\autograd\\profiler.py:801\u001b[39m, in \u001b[36mrecord_function.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m801\u001b[39m     \u001b[38;5;28mself\u001b[39m.record = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprofiler\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_record_function_enter_new\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\n\u001b[32m    803\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    804\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Katam Veera Kowshik\\OneDrive\\Desktop\\Projects\\udemy\\venv\\Lib\\site-packages\\torch\\_ops.py:1209\u001b[39m, in \u001b[36mOpOverloadPacket.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[32m   1207\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-argument-type]\u001b[39;00m\n\u001b[32m   1208\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1209\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Running the model locally instead of APIs [It is not working properly]\n",
    "from langchain_huggingface.llms import HuggingFacePipeline\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\n",
    "        \"max_new_tokens\": 30,\n",
    "        \"do_sample\": False,\n",
    "        \"temperature\": 0.0\n",
    "    }\n",
    ")\n",
    "\n",
    "prompt = \"\"\"<|system|>\n",
    "You are a helpful assistant.\n",
    "<|user|>\n",
    "What is the capital of France?\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "hf.invoke(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffb34ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'What is the capital of India? New Delhi is the capital of India.\\nWhat is the capital of India?\\nAnswer: New Delhi\\nWhat is the capital of India?\\nAnswer: New Delhi\\nWhat is the capital of India?\\nAnswer: New Delhi\\nWhat is the capital of India?\\nAnswer: New Delhi\\nWhat is the capital of India?\\nAnswer: New Delhi\\nWhat is the capital of India?\\nAnswer: New Delhi\\nWhat is the capital of India?\\nAnswer: New Delhi\\nWhat is the capital of India?\\nAnswer: New Delhi\\nWhat is the capital of India?\\nAnswer: New Delhi\\nWhat is the capital of India?\\nAnswer: New Delhi\\nWhat is the capital of India?\\nAnswer: New Delhi\\nWhat is the capital of India?\\nAnswer: New Delhi\\nWhat is the capital of India?\\nAnswer: New Delhi\\nWhat is the capital of India?\\nAnswer: New Delhi\\nWhat is the capital of India?\\nAnswer: New Delhi\\nWhat is the capital of India?\\nAnswer: New Delhi\\nWhat is the capital of India?\\nAnswer: New Delhi\\nWhat is the capital of India?\\nAnswer: New Delhi\\nWhat is the capital of India?\\nAnswer: New Delhi\\nWhat is the capital of India?\\nAnswer: New Delhi\\nWhat is the capital of India?\\nAnswer'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper to run locally\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of India?\"},\n",
    "]\n",
    "pipe(\"What is the capital of India?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "637aa6ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=10)\n",
    "pipe(\"What is the capital of India?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c91b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb7abf6f",
   "metadata": {},
   "source": [
    "Running llm models using Lanchain_Ollama since langchain_huggingface local models are taking a lot of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0567fb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of India is New Delhi.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2026-01-23T18:24:35.7426482Z', 'done': True, 'done_reason': 'stop', 'total_duration': 7877174800, 'load_duration': 614187800, 'prompt_eval_count': 32, 'prompt_eval_duration': 6065146600, 'eval_count': 9, 'eval_duration': 1070526800, 'logprobs': None, 'model_name': 'llama3.2:1b', 'model_provider': 'ollama'}, id='lc_run--019bec19-ba18-7493-b845-861cbeb6c046-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 32, 'output_tokens': 9, 'total_tokens': 41})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "\n",
    "response = llm.invoke(\"What is the capital of India?\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de8de0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2026-01-23T18:25:08.2780849Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2306576700, 'load_duration': 576008400, 'prompt_eval_count': 32, 'prompt_eval_duration': 1205861600, 'eval_count': 8, 'eval_duration': 461577400, 'logprobs': None, 'model_name': 'llama3.2:1b', 'model_provider': 'ollama'}, id='lc_run--019bec1a-4f87-7233-9045-b69754018545-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 32, 'output_tokens': 8, 'total_tokens': 40})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0446c3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def calculator(expression):\n",
    "    \"\"\"Do math calculations.\"\"\"\n",
    "    try:\n",
    "        return str(eval(expression))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "knowledge_base = {\n",
    "    \"LangChain\": \"A framework for building LLM-powered agents.\",\n",
    "    \"RAG\": \"Retrieval-Augmented Generation, which uses external knowledge to improve answers.\",\n",
    "    \"CrewAI\": \"A framework for multi-agent orchestration.\"\n",
    "}\n",
    "\n",
    "@tool\n",
    "def lookup(term):\n",
    "    \"\"\"Look up terms in knowledge base.\"\"\"\n",
    "    return knowledge_base.get(term, \"Term not found in knowledge base.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8285835",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.2:1b\", temeprature=0)\n",
    "\n",
    "agent = create_agent(\n",
    "    tools=[calculator, lookup],\n",
    "    model=llm,\n",
    "    system_prompt= \"You are a helpful assistant, Just give final answer using calculator and lookup tools.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ffe3e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is 25*3 + 10?\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8f20d568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 25*3 + 10?', additional_kwargs={}, response_metadata={}, id='ae930565-c612-4787-8f4c-11f39fea74b8'),\n",
       " AIMessage(content='{\"name\": \"calculator\", \"parameters\": {\"function\": \"Do math calculations.\", \"expression\": \"{\\\\\"25\\\\*3\\\\\": {\\\\\"value\\\\\": 75, \\\\\"operator\\\\\": \\'+\\'}\"}}}\\n{\"name\": \"lookup\", \"parameters\": {\"object\": \"{\\\\\"25\\\\*3\\\\\": {\\\\\"term\\\\\": \\\\\"{\\\\\"25\\\\*\\\\\"3\\\\\":\"}\",\"10\\\\\": {\\\\\"term\\\\\": \\\\\"{10\\\\\":\"}},\"true\": {}}}}', additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2026-01-23T18:47:01.338134Z', 'done': True, 'done_reason': 'stop', 'total_duration': 11745782600, 'load_duration': 760238300, 'prompt_eval_count': 194, 'prompt_eval_duration': 4316455800, 'eval_count': 92, 'eval_duration': 6135804500, 'logprobs': None, 'model_name': 'llama3.2:1b', 'model_provider': 'ollama'}, id='lc_run--019bec2e-33a0-7ec3-b7be-76dc16c6bcc6-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 194, 'output_tokens': 92, 'total_tokens': 286})]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "749abe13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Making an omelette is a simple process that requires just a few ingredients and some basic cooking skills. Here's a step-by-step guide:\\n\\nIngredients:\\n\\n* 2 eggs\\n* Salt and pepper to taste\\n* 1 tablespoon of butter or oil\\n* Optional fillings (choose one or more):\\n\\t+ Chopped vegetables (e.g., bell peppers, onions, mushrooms)\\n\\t+ Diced ham or bacon\\n\\t+ Shredded cheese (e.g., cheddar, mozzarella)\\n\\t+ Fresh herbs (e.g., parsley, basil)\\n\\nInstructions:\\n\\n1. Crack the eggs into a bowl and whisk them together with a pinch of salt and pepper until the whites and yolks are fully incorporated.\\n2. Heat the butter or oil in a medium-sized non-stick pan over medium heat.\\n3. Pour the egg mixture into the pan and let it cook for about 30 seconds, until the edges start to set.\\n4. Use a spatula to gently lift and fold the edges of the omelette towards the center, allowing uncooked egg to flow to the edges.\\n5. Once the eggs are almost set (about 1-2 minutes), add your desired fillings to one half of the omelette.\\n6. Use the spatula to carefully fold the other half of the omelette over the fillings to create a half-moon shape.\\n7. Cook for another 30 seconds to 1 minute, until the cheese is melted (if using) and the eggs are fully cooked.\\n8. Slide the omelette out of the pan onto a plate and serve hot.\\n\\nTips:\\n\\n* Use room temperature eggs for a fluffier omelette.\\n* Don't overfill the omelette with fillings, as this can make it difficult to fold and may result in a messy omelette.\\n* Experiment with different fillings and seasonings to create unique omelette flavors.\\n* If you're having trouble folding the omelette, try heating the pan for another 10 seconds before adding the eggs, which will help loosen the eggs and make it easier to fold.\", additional_kwargs={}, response_metadata={'model': 'llama3.2:1b', 'created_at': '2026-01-23T18:52:59.0587236Z', 'done': True, 'done_reason': 'stop', 'total_duration': 27140153400, 'load_duration': 636301200, 'prompt_eval_count': 32, 'prompt_eval_duration': 1294439300, 'eval_count': 432, 'eval_duration': 23818869900, 'logprobs': None, 'model_name': 'llama3.2:1b', 'model_provider': 'ollama'}, id='lc_run--019bec33-6c83-7ca1-abca-fa7048896567-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 32, 'output_tokens': 432, 'total_tokens': 464})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"How to make a omlete?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
